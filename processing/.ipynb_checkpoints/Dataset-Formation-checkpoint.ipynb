{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import pdb\n",
    "import time\n",
    "import gc\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Formation\n",
    "\n",
    "In the other notebooks, I pre-processed the captions for each image, and created an \"image embedding\" by extracting an intermediate representation from a pre-trained deep neural network called VGG16. \n",
    "\n",
    "Now, I will process each image/caption pair to create a dataset that can be used to train an LSTM. This will require fitting a tokenizer on the captions, and converting each caption into a set of training examples, where each word is to be predicted by its predecesessors. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load image features and captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/features/train_features.pkl\", \"rb\") as handle:\n",
    "    train_features = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/features/valid_features.pkl\", \"rb\") as handle:\n",
    "    valid_features = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the captions\n",
    "train_captions = pd.read_csv(\"../data/split_lists/train_ids.csv\", dtype = str)\n",
    "valid_captions = pd.read_csv(\"../data/split_lists/valid_ids.csv\", dtype = str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>photo_id</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_ExrVJTjGcChfzLH51etAw</td>\n",
       "      <td>shanghai rainbow trout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yPUPhsJvT6yx6l8QwShw1Q</td>\n",
       "      <td>grill rainbow trout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zvESg-w2JIBL5FhU7F2d-g</td>\n",
       "      <td>chicken parm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uqdXqfB8MXW6XU7Hk1gGIQ</td>\n",
       "      <td>mcg holiday jazz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VMedbsDZnCxmCE3Pndvtng</td>\n",
       "      <td>dining room</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 photo_id                 caption\n",
       "0  _ExrVJTjGcChfzLH51etAw  shanghai rainbow trout\n",
       "1  yPUPhsJvT6yx6l8QwShw1Q     grill rainbow trout\n",
       "2  zvESg-w2JIBL5FhU7F2d-g            chicken parm\n",
       "3  uqdXqfB8MXW6XU7Hk1gGIQ        mcg holiday jazz\n",
       "4  VMedbsDZnCxmCE3Pndvtng             dining room"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_captions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# does everything make sense, in terms of shapes? \n",
    "print(valid_captions.shape[0] == len(valid_features))\n",
    "print(train_captions.shape[0] == len(train_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build a Tokenizer\n",
    "\n",
    "Now we need to build a tokenizer, so that we can vectorize our words in a consistent way. \n",
    "First, I'll add start and end tokens to the begining of each caption to mark the start/end of the caption. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_captions.caption = \"startseq \" + train_captions.caption + \" endseq\"\n",
    "valid_captions.caption = \"startseq \" + valid_captions.caption + \" endseq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>photo_id</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lHhMNhCA7rAZmi-MMfF3ZA</td>\n",
       "      <td>startseq bakery area endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kHrkA-8BY4tC-rejiJNhBQ</td>\n",
       "      <td>startseq the hangars house band tuned up check...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ljStFDx0XFg8jSbJIRhvGA</td>\n",
       "      <td>startseq thats the mans butt ask him about bee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JoB_QTE2Hjr1NT0AgYdWzQ</td>\n",
       "      <td>startseq bbq shrimp endseq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PvdNq213kqk7N9raOcefEw</td>\n",
       "      <td>startseq sharkfin pie endseq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 photo_id                                            caption\n",
       "0  lHhMNhCA7rAZmi-MMfF3ZA                        startseq bakery area endseq\n",
       "1  kHrkA-8BY4tC-rejiJNhBQ  startseq the hangars house band tuned up check...\n",
       "2  ljStFDx0XFg8jSbJIRhvGA  startseq thats the mans butt ask him about bee...\n",
       "3  JoB_QTE2Hjr1NT0AgYdWzQ                         startseq bbq shrimp endseq\n",
       "4  PvdNq213kqk7N9raOcefEw                       startseq sharkfin pie endseq"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_captions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a tokenizer \n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_captions = np.concatenate([train_captions.caption.values,valid_captions.caption.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit tokenizer\n",
    "tokenizer.fit_on_texts(all_captions.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30212"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store the vocabulary size\n",
    "vocab_size = 1 + len(tokenizer.word_index)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir ../data/tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/tokenizer/tokenizer.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(tokenizer, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build a consolidated dataset. \n",
    "\n",
    "To build a language model-type network, we need to convert each sequence into a set of training examples for each caption. for example, the sentence _little girl is running in field_, along with the feature of the corresponding photo, will be transformed into the following sets of training examples: \n",
    "\n",
    "```\n",
    "X1,\t\t    X2 (input sequence), \t\t\t\t\t\ty (target)\n",
    "photo-features\tstartseq, \t\t\t\t\t\t\t\t\tlittle\n",
    "photo-features\tstartseq, little,\t\t\t\t\t\t\tgirl\n",
    "photo-features\tstartseq, little, girl, \t\t\t\t\trunning\n",
    "photo-features\tstartseq, little, girl, running, \t\t\tin\n",
    "photo-features\tstartseq, little, girl, running, in, \t\tfield\n",
    "photo-features\tstartseq, little, girl, running, in, field, endseq\n",
    "```\n",
    "\n",
    "This example is taken from [Dr. Brownlee's awesome blog. ](https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_pad(caption, sequence_length = 15):\n",
    "    # questions encoded as index vectors\n",
    "    encoded = tokenizer.texts_to_sequences([caption])\n",
    "    # padded squences to be of length [sequence_length]\n",
    "    padded = pad_sequences(encoded, \n",
    "                            maxlen = sequence_length,\n",
    "                            padding = \"post\", \n",
    "                            truncating = \"post\")[0]\n",
    "    return(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(caption):\n",
    "    # questions encoded as index vectors\n",
    "    encoded = tokenizer.texts_to_sequences([caption])[0]\n",
    "    return (encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a dictionary of {photo_id : photo-featres} pairs and a dataframe of captions into two numpy arrays\n",
    "# that can be used as a consolidated training dataset\n",
    "def consolidate_dataset(features_dict, captions_df, sequence_length = 15):\n",
    "    # keep track of the photo features and caption sequenes in lists\n",
    "    X_photos, X_captions = [], []\n",
    "    y = [] # build response vector\n",
    "    e = 0\n",
    "    prevtime = time.time()\n",
    "    for photo_id in captions_df['photo_id']:\n",
    "        # if the photo_id is not in the feature dictionary, move on\n",
    "        if photo_id not in features_dict:\n",
    "            continue\n",
    "        if e % 1000 == 0:\n",
    "            print (\"reached %d in %f sec\" % (e, time.time() - prevtime))\n",
    "            prevtime = time.time()\n",
    "        e += 1\n",
    "        \"\"\"\n",
    "        For each word in the caption, add a copy of the photo features to the features list,\n",
    "        as well as the vectorization of the caption up to and including the current word\n",
    "        \"\"\"\n",
    "        current_feature = features_dict[photo_id][0]\n",
    "        current_caption = str(captions_df.loc[captions_df.photo_id == photo_id].iloc[0][\"caption\"])\n",
    "        current_caption_split = current_caption.split()\n",
    "        for i in range(1,len(current_caption.split())):\n",
    "            # add a copy of the photo features\n",
    "            X_photos.append(current_feature)\n",
    "            # encode the input and output sequence\n",
    "            in_words, out_word = \" \".join(current_caption_split[:i]), current_caption_split[i]\n",
    "            in_seq = encode_and_pad(in_words, sequence_length = sequence_length)\n",
    "            # add the training sequences and responses to list\n",
    "            X_captions.append(in_seq)\n",
    "            y.append(encode(out_word))\n",
    "    # return all three\n",
    "    return(X_photos, X_captions, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, running this function to get our training data and labels, as well as our validation data and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached 0 in 0.000296 sec\n",
      "reached 1000 in 3.907506 sec\n",
      "reached 2000 in 3.458567 sec\n",
      "reached 3000 in 4.025187 sec\n",
      "reached 4000 in 3.572920 sec\n",
      "reached 5000 in 3.725117 sec\n",
      "reached 6000 in 5.110378 sec\n",
      "reached 7000 in 4.129266 sec\n",
      "reached 8000 in 4.464129 sec\n",
      "reached 9000 in 4.558153 sec\n",
      "reached 10000 in 3.578678 sec\n",
      "reached 11000 in 3.458837 sec\n",
      "reached 12000 in 3.604255 sec\n",
      "reached 13000 in 3.849694 sec\n",
      "reached 14000 in 4.159780 sec\n",
      "reached 15000 in 4.086161 sec\n",
      "reached 16000 in 4.044107 sec\n",
      "reached 17000 in 3.463907 sec\n",
      "reached 18000 in 3.289455 sec\n",
      "reached 19000 in 3.366791 sec\n",
      "reached 20000 in 3.502182 sec\n"
     ]
    }
   ],
   "source": [
    "X_valid_photos, X_valid_captions, y_valid = consolidate_dataset(valid_features, valid_captions, sequence_length=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached 0 in 0.340574 sec\n",
      "reached 1000 in 10.293230 sec\n",
      "reached 2000 in 10.069726 sec\n",
      "reached 3000 in 10.746813 sec\n",
      "reached 4000 in 10.988190 sec\n",
      "reached 5000 in 12.416748 sec\n",
      "reached 6000 in 22.106808 sec\n",
      "reached 7000 in 11.373367 sec\n",
      "reached 8000 in 10.917947 sec\n",
      "reached 9000 in 10.799975 sec\n",
      "reached 10000 in 11.161716 sec\n",
      "reached 11000 in 11.027584 sec\n",
      "reached 12000 in 11.317970 sec\n",
      "reached 13000 in 10.280054 sec\n",
      "reached 14000 in 10.294515 sec\n",
      "reached 15000 in 10.376237 sec\n",
      "reached 16000 in 10.167927 sec\n",
      "reached 17000 in 11.045395 sec\n",
      "reached 18000 in 10.711648 sec\n",
      "reached 19000 in 10.562855 sec\n",
      "reached 20000 in 10.798727 sec\n",
      "reached 21000 in 10.465860 sec\n",
      "reached 22000 in 10.307129 sec\n",
      "reached 23000 in 10.314936 sec\n",
      "reached 24000 in 10.243981 sec\n",
      "reached 25000 in 10.410780 sec\n",
      "reached 26000 in 12.681155 sec\n",
      "reached 27000 in 11.305082 sec\n",
      "reached 28000 in 10.430054 sec\n",
      "reached 29000 in 11.315182 sec\n",
      "reached 30000 in 13.327277 sec\n",
      "reached 31000 in 9.614445 sec\n",
      "reached 32000 in 9.542184 sec\n",
      "reached 33000 in 10.859425 sec\n",
      "reached 34000 in 10.296913 sec\n",
      "reached 35000 in 9.654857 sec\n",
      "reached 36000 in 9.434523 sec\n",
      "reached 37000 in 9.498368 sec\n",
      "reached 38000 in 9.375901 sec\n",
      "reached 39000 in 9.369930 sec\n",
      "reached 40000 in 9.405281 sec\n",
      "reached 41000 in 9.605289 sec\n",
      "reached 42000 in 9.376771 sec\n",
      "reached 43000 in 9.376825 sec\n",
      "reached 44000 in 9.497400 sec\n",
      "reached 45000 in 9.513961 sec\n",
      "reached 46000 in 9.493375 sec\n",
      "reached 47000 in 9.560392 sec\n",
      "reached 48000 in 9.441099 sec\n",
      "reached 49000 in 9.373886 sec\n",
      "reached 50000 in 9.369637 sec\n",
      "reached 51000 in 9.405593 sec\n",
      "reached 52000 in 9.460971 sec\n",
      "reached 53000 in 9.332916 sec\n",
      "reached 54000 in 9.438169 sec\n",
      "reached 55000 in 9.495492 sec\n",
      "reached 56000 in 9.456992 sec\n",
      "reached 57000 in 9.870991 sec\n",
      "reached 58000 in 10.188102 sec\n",
      "reached 59000 in 9.951161 sec\n",
      "reached 60000 in 9.453946 sec\n",
      "reached 61000 in 9.560415 sec\n",
      "reached 62000 in 9.519595 sec\n",
      "reached 63000 in 9.542957 sec\n",
      "reached 64000 in 9.544574 sec\n",
      "reached 65000 in 9.478330 sec\n",
      "reached 66000 in 9.478571 sec\n",
      "reached 67000 in 9.578904 sec\n",
      "reached 68000 in 9.559858 sec\n",
      "reached 69000 in 9.481085 sec\n",
      "reached 70000 in 9.437066 sec\n",
      "reached 71000 in 9.700252 sec\n",
      "reached 72000 in 9.565020 sec\n",
      "reached 73000 in 10.037386 sec\n",
      "reached 74000 in 9.459859 sec\n",
      "reached 75000 in 9.439941 sec\n",
      "reached 76000 in 9.509579 sec\n",
      "reached 77000 in 9.445523 sec\n",
      "reached 78000 in 9.487776 sec\n",
      "reached 79000 in 9.471200 sec\n",
      "reached 80000 in 9.496701 sec\n"
     ]
    }
   ],
   "source": [
    "X_train_photos, X_train_captions, y_train = consolidate_dataset(train_features, train_captions, sequence_length=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Convert and save as numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_photos = np.array(X_valid_photos, dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_captions = np.array(X_valid_captions, dtype = np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid = np.array(y_valid, dtype = np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir ../data/preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_npy(path, arr):\n",
    "    with open(path, \"wb\") as handle:\n",
    "        np.save(path, arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_npy(\"../data/preprocessed/X_valid_photos.npy\", X_valid_photos)\n",
    "save_npy(\"../data/preprocessed/X_valid_captions.npy\", X_valid_captions)\n",
    "# save_npy(\"../data/preprocessed/y_valid.npy\", y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_photos = np.array(X_train_photos, dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_captions= np.array(X_train_captions, dtype = np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train, np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_npy(\"../data/preprocessed/X_train_photos.npy\", X_train_photos)\n",
    "save_npy(\"../data/preprocessed/X_train_captions.npy\", X_train_captions)\n",
    "# save_npy(\"../data/preprocessed/y_train.npy\", y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save an embedding matrix\n",
    "\n",
    "Finally, I'll load and save an embedding matrix (with pretrained word2vec vectors) as a numpy array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = KeyedVectors.load_word2vec_format('~/Desktop/embeddings/word2vec/GoogleNews-vectors-negative300.bin',\n",
    "                                                   binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each row in the matrix is the embedding of one word in the joint datasets. \n",
    "# The row index corresponds to the integer ecoding of that word. \n",
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in embedding_model:\n",
    "        embedding_matrix[i] = embedding_model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.109375  ,  0.140625  , -0.03173828,  0.16601562, -0.07128906,\n",
       "        0.01586914, -0.00311279, -0.08496094, -0.04858398,  0.05566406,\n",
       "       -0.08251953, -0.02404785, -0.00665283,  0.03686523, -0.05029297,\n",
       "       -0.02941895,  0.11376953, -0.06787109,  0.05639648, -0.07568359,\n",
       "       -0.03857422,  0.09716797, -0.04418945, -0.12207031,  0.140625  ,\n",
       "        0.08496094,  0.09667969,  0.07470703, -0.0039978 ,  0.17285156,\n",
       "       -0.06933594,  0.08886719,  0.03808594, -0.00061417,  0.01184082,\n",
       "        0.00164032, -0.20898438, -0.08251953,  0.08984375,  0.07910156,\n",
       "        0.12353516, -0.01867676,  0.03039551,  0.04711914,  0.06542969,\n",
       "       -0.01251221,  0.00152588,  0.10644531, -0.01531982, -0.04199219,\n",
       "        0.16796875,  0.05175781,  0.07470703,  0.08251953,  0.01721191,\n",
       "        0.01599121,  0.02734375, -0.03686523,  0.08105469, -0.06445312,\n",
       "       -0.08984375,  0.10742188,  0.01153564, -0.13671875,  0.05151367,\n",
       "       -0.02429199,  0.02282715,  0.12353516,  0.01531982,  0.1484375 ,\n",
       "        0.02441406, -0.0625    ,  0.08154297, -0.06494141,  0.02038574,\n",
       "        0.05126953,  0.125     ,  0.09765625,  0.05810547,  0.18652344,\n",
       "        0.05834961, -0.19921875,  0.02001953, -0.04638672, -0.12158203,\n",
       "       -0.12109375, -0.02429199,  0.22851562, -0.05712891,  0.03051758,\n",
       "        0.11816406, -0.05712891, -0.125     , -0.21972656, -0.06152344,\n",
       "       -0.11865234,  0.11816406,  0.01196289, -0.08398438,  0.04858398,\n",
       "       -0.05053711, -0.07568359,  0.02819824, -0.05371094, -0.14160156,\n",
       "        0.07568359, -0.18164062, -0.18847656, -0.09179688, -0.15234375,\n",
       "       -0.06005859, -0.03833008, -0.0111084 ,  0.05541992,  0.07226562,\n",
       "       -0.0098877 ,  0.15820312, -0.04003906,  0.1328125 ,  0.1015625 ,\n",
       "       -0.03930664, -0.09472656, -0.14550781,  0.01818848, -0.10302734,\n",
       "        0.02453613, -0.01831055, -0.03125   , -0.03710938,  0.00126648,\n",
       "       -0.19042969, -0.09423828, -0.20117188, -0.14257812,  0.06176758,\n",
       "       -0.13671875,  0.06152344,  0.01330566,  0.04223633,  0.15722656,\n",
       "        0.06884766, -0.09082031,  0.1015625 , -0.11669922,  0.02612305,\n",
       "       -0.02270508, -0.04321289, -0.01867676, -0.03149414, -0.05688477,\n",
       "        0.05175781, -0.06933594, -0.15527344,  0.08789062, -0.04174805,\n",
       "        0.07275391,  0.18261719, -0.18066406, -0.00393677, -0.09521484,\n",
       "       -0.07177734,  0.0390625 , -0.03271484,  0.08544922,  0.06835938,\n",
       "       -0.12109375,  0.19921875, -0.05517578, -0.03759766,  0.10644531,\n",
       "       -0.0300293 , -0.08203125, -0.00915527, -0.12304688, -0.01403809,\n",
       "       -0.07177734,  0.09423828, -0.05200195,  0.0559082 , -0.05200195,\n",
       "       -0.02636719,  0.05834961, -0.11669922,  0.05371094, -0.00166321,\n",
       "       -0.04248047, -0.1875    , -0.04736328,  0.23046875,  0.17382812,\n",
       "        0.00343323,  0.07617188, -0.00540161,  0.012146  ,  0.0100708 ,\n",
       "       -0.02600098,  0.02929688,  0.13867188, -0.00753784, -0.08984375,\n",
       "        0.01556396,  0.06835938, -0.09619141,  0.19335938, -0.10058594,\n",
       "        0.01977539,  0.02502441, -0.01501465,  0.10449219,  0.06933594,\n",
       "        0.05761719,  0.14746094,  0.10595703,  0.04467773, -0.2109375 ,\n",
       "       -0.00317383,  0.15527344, -0.0546875 , -0.09179688, -0.0189209 ,\n",
       "       -0.04980469, -0.0189209 , -0.14550781, -0.04931641,  0.19433594,\n",
       "       -0.08007812,  0.10986328,  0.04077148,  0.05322266,  0.10498047,\n",
       "       -0.08837891, -0.11230469,  0.16601562,  0.05517578, -0.03637695,\n",
       "       -0.08203125, -0.0378418 ,  0.09082031,  0.11572266,  0.14941406,\n",
       "       -0.0177002 , -0.01489258, -0.03063965, -0.08105469, -0.07470703,\n",
       "        0.00491333,  0.10058594,  0.03015137, -0.02539062,  0.0135498 ,\n",
       "       -0.08349609,  0.00753784,  0.01361084,  0.03149414,  0.00946045,\n",
       "        0.01525879,  0.21777344, -0.01293945, -0.13867188, -0.09716797,\n",
       "        0.10498047,  0.02294922, -0.05444336, -0.00476074,  0.07128906,\n",
       "        0.13964844,  0.12158203, -0.03564453, -0.14550781,  0.00069427,\n",
       "        0.140625  ,  0.19433594,  0.10888672,  0.05493164,  0.08642578,\n",
       "       -0.01806641,  0.02502441, -0.06835938,  0.03637695, -0.06591797,\n",
       "       -0.03930664,  0.07666016,  0.03979492,  0.15234375,  0.11425781,\n",
       "       -0.02770996, -0.1328125 , -0.1484375 , -0.05273438, -0.05004883,\n",
       "       -0.06494141,  0.00405884,  0.01055908,  0.06445312,  0.01525879,\n",
       "       -0.07666016,  0.00366211,  0.00765991,  0.12011719, -0.1796875 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model[\"this\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir ../data/embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_npy(\"../data/embedding_matrix/embedding_matrix.npy\", embedding_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
