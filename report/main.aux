\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{vinyals}
\citation{tanti1}
\citation{tanti2}
\citation{cho}
\citation{kumar}
\citation{xu}
\citation{8k}
\citation{30k}
\citation{coco}
\citation{wu}
\citation{vgg}
\citation{keras}
\citation{imagenet}
\citation{zipf}
\citation{word2vec}
\@writefile{toc}{\contentsline {section}{\numberline {I}INTRODUCTION}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}RELATED WORK}{1}{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {III}THEORETICAL BACKGROUND}{2}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {III-A}Training as Maximizing Caption Likelihood}{2}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {III-B}Encoder-Decoder Framework for Image Captioning}{2}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {III-C}Merge and Inject Models}{2}{subsection.3.3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:test1}{{\caption@xref {fig:test1}{ on input line 407}}{3}{Encoder-Decoder Framework for Image Captioning}{figure.caption.1}{}}
\newlabel{fig:test2}{{\caption@xref {fig:test2}{ on input line 412}}{3}{Encoder-Decoder Framework for Image Captioning}{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The encoder-decoder framework in neural machine translation models (left) and image captioning models (right). In machine translation applications, an input sequence is encoded by an RNN to a vector represention, before decoded to an output sequence. Analogously, a neural image captioning system uses a CNN to encode an input image before generating an output sequence.\relax }}{3}{figure.caption.1}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}The Yelp Dataset}{3}{section.4}}
\newlabel{fig:test1}{{\caption@xref {fig:test1}{ on input line 447}}{4}{Merge and Inject Models}{figure.caption.2}{}}
\newlabel{fig:test2}{{\caption@xref {fig:test2}{ on input line 452}}{4}{Merge and Inject Models}{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A bare-bones diagram of the Inject architecture (left) and Merge architecture (right). The former conditions the RNN with both the image and linguistic features, and uses the RNN to generate new tokens. The uses the RNN to encode the previously predicted tokens into a vector representation, rather than to generate new tokens.\relax }}{4}{figure.caption.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Instructions presented to Amazon Turkers in the collection of the Flickr-8k dataset (Rashtchian et al., 2010). Image extracted directly from [7].\relax }}{4}{figure.caption.3}}
\newlabel{fig:test1}{{3}{4}{Instructions presented to Amazon Turkers in the collection of the Flickr-8k dataset (Rashtchian et al., 2010). Image extracted directly from [7].\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}METHODOLOGY}{4}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {V-A}Extracting Image Embeddings}{4}{subsection.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces list=off}}{4}{figure.caption.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Vector representations of images, or ``image embeddings", are created by saving an intermediate activation of the VGG16 network, which is capable of detecting rich perceptual features, such as edges, blur and hue. These embeddings, combined with linguistic features, are used to generate captions for input images. Thus, the abridged VGG16 network may be thought of as the \emph  {encoder} in the \emph  {Encoder-Decoder} framework.\relax }}{5}{figure.caption.5}}
\newlabel{fig:test1}{{5}{5}{Vector representations of images, or ``image embeddings", are created by saving an intermediate activation of the VGG16 network, which is capable of detecting rich perceptual features, such as edges, blur and hue. These embeddings, combined with linguistic features, are used to generate captions for input images. Thus, the abridged VGG16 network may be thought of as the \emph {encoder} in the \emph {Encoder-Decoder} framework.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {V-B}Caption Text Preprocessing}{5}{subsection.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {V-C}Formatting Data for Training}{5}{subsection.5.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces An example of how an image-embedding/caption pair is reformatted to suit a language modeling task. Each token (except for the first \texttt  {<startseq>} token is treated as a response, where the covariates are the previous tokens in the sequence, and a copy of the image embedding.\relax }}{6}{figure.caption.6}}
\newlabel{fig:test1}{{6}{6}{An example of how an image-embedding/caption pair is reformatted to suit a language modeling task. Each token (except for the first \texttt {<startseq>} token is treated as a response, where the covariates are the previous tokens in the sequence, and a copy of the image embedding.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {V-D}Model Architectures}{6}{subsection.5.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {V-E}Inference as Beam Search}{6}{subsection.5.5}}
\newlabel{fig:test1}{{\caption@xref {fig:test1}{ on input line 570}}{7}{Model Architectures}{figure.caption.7}{}}
\newlabel{fig:test2}{{\caption@xref {fig:test2}{ on input line 575}}{7}{Model Architectures}{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Inject model (left) and merge-concat models (right). In the inject model, image features are treated as the first ``word" in the caption, then passed to the LSTM cell. In the merge-concat model, image and caption features are processed seperately, concatenated, then passed to a dense layer to form predictions.   The merge-add layer (not shown above) differs from the merge-concat model only in that image and caption features are \textit  {added} instead of \textit  {concatenated}.   Layer names above refer to \href  {https://keras.io/layers/about-keras-layers/}{\texttt  {keras} layer classes}.\relax }}{7}{figure.caption.7}}
\bibstyle{IEEEtran}
\bibdata{main}
\bibcite{vinyals}{1}
\bibcite{tanti1}{2}
\bibcite{tanti2}{3}
\bibcite{cho}{4}
\bibcite{kumar}{5}
\bibcite{xu}{6}
\bibcite{8k}{7}
\bibcite{30k}{8}
\bibcite{coco}{9}
\bibcite{wu}{10}
\bibcite{vgg}{11}
\bibcite{keras}{12}
\bibcite{imagenet}{13}
\bibcite{zipf}{14}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Inference: Greedy Selection\relax }}{8}{algorithm.1}}
\newlabel{greedy-sampling}{{1}{8}{Inference: Greedy Selection\relax }{algorithm.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Inference: Beam Search\relax }}{8}{algorithm.2}}
\newlabel{beam-search}{{2}{8}{Inference: Beam Search\relax }{algorithm.2}{}}
\@writefile{toc}{\contentsline {section}{References}{8}{section*.9}}
\bibcite{word2vec}{15}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Top caption predictions using beam search and the inject model, for different values of $\alpha $. Small values of $\alpha $ lead to overly terse captions, while large values of $\alpha $ lead to wordy and repetitive captions.\relax }}{9}{figure.caption.8}}
\newlabel{fig:test1}{{8}{9}{Top caption predictions using beam search and the inject model, for different values of $\alpha $. Small values of $\alpha $ lead to overly terse captions, while large values of $\alpha $ lead to wordy and repetitive captions.\relax }{figure.caption.8}{}}
