\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{vinyals}
\citation{tanti1}
\citation{tanti2}
\citation{cho}
\citation{kumar}
\citation{xu}
\citation{8k}
\citation{30k}
\citation{coco}
\citation{wu}
\citation{vgg}
\citation{keras}
\citation{imagenet}
\citation{zipf}
\citation{word2vec}
\citation{bleu}
\citation{rouge}
\citation{cider}
\@writefile{toc}{\contentsline {section}{\numberline {I}INTRODUCTION}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}RELATED WORK}{1}{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {III}THEORETICAL BACKGROUND}{2}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {III-A}Training as Maximizing Caption Likelihood}{2}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {III-B}Encoder-Decoder Framework for Image Captioning}{2}{subsection.3.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:test1}{{\caption@xref {fig:test1}{ on input line 467}}{3}{Encoder-Decoder Framework for Image Captioning}{figure.caption.1}{}}
\newlabel{fig:test2}{{\caption@xref {fig:test2}{ on input line 472}}{3}{Encoder-Decoder Framework for Image Captioning}{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The encoder-decoder framework in neural machine translation models (left) and image captioning models (right). In machine translation applications, an input sequence is encoded by an RNN to a vector represention, before decoded to an output sequence. Analogously, a neural image captioning system uses a CNN to encode an input image before generating an output sequence.\relax }}{3}{figure.caption.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {III-C}Merge and Inject Models}{3}{subsection.3.3}}
\newlabel{fig:test1}{{\caption@xref {fig:test1}{ on input line 507}}{4}{Merge and Inject Models}{figure.caption.2}{}}
\newlabel{fig:test2}{{\caption@xref {fig:test2}{ on input line 512}}{4}{Merge and Inject Models}{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A bare-bones diagram of the Inject architecture (left) and Merge architecture (right). The former conditions the RNN with both the image and linguistic features, and uses the RNN to generate new tokens. The uses the RNN to encode the previously predicted tokens into a vector representation, rather than to generate new tokens.\relax }}{4}{figure.caption.2}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}The Yelp Dataset}{4}{section.4}}
\@writefile{toc}{\contentsline {section}{\numberline {V}METHODOLOGY}{4}{section.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Instructions presented to Amazon Turkers in the collection of the Flickr-8k dataset (Rashtchian et al., 2010). Image extracted directly from [7].\relax }}{4}{figure.caption.3}}
\newlabel{fig:test1}{{3}{4}{Instructions presented to Amazon Turkers in the collection of the Flickr-8k dataset (Rashtchian et al., 2010). Image extracted directly from [7].\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces list=off}}{4}{figure.caption.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {V-A}Extracting Image Embeddings}{5}{subsection.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {V-B}Caption Text Preprocessing}{5}{subsection.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {V-C}Formatting Data for Training}{5}{subsection.5.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Vector representations of images, or ``image embeddings", are created by saving an intermediate activation of the VGG16 network, which is capable of detecting rich perceptual features, such as edges, blur and hue. These embeddings, combined with linguistic features, are used to generate captions for input images. Thus, the abridged VGG16 network may be thought of as the \emph  {encoder} in the \emph  {Encoder-Decoder} framework.\relax }}{6}{figure.caption.5}}
\newlabel{fig:test1}{{5}{6}{Vector representations of images, or ``image embeddings", are created by saving an intermediate activation of the VGG16 network, which is capable of detecting rich perceptual features, such as edges, blur and hue. These embeddings, combined with linguistic features, are used to generate captions for input images. Thus, the abridged VGG16 network may be thought of as the \emph {encoder} in the \emph {Encoder-Decoder} framework.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces An example of how an image-embedding/caption pair is reformatted to suit a language modeling task. Each token (except for the first \texttt  {<startseq>} token is treated as a response, where the covariates are the previous tokens in the sequence, and a copy of the image embedding.\relax }}{6}{figure.caption.6}}
\newlabel{fig:test1}{{6}{6}{An example of how an image-embedding/caption pair is reformatted to suit a language modeling task. Each token (except for the first \texttt {<startseq>} token is treated as a response, where the covariates are the previous tokens in the sequence, and a copy of the image embedding.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {V-D}Model Architectures}{6}{subsection.5.4}}
\newlabel{fig:test1}{{\caption@xref {fig:test1}{ on input line 628}}{7}{Model Architectures}{figure.caption.7}{}}
\newlabel{fig:test2}{{\caption@xref {fig:test2}{ on input line 633}}{7}{Model Architectures}{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Inject model (left) and merge-concat models (right). In the inject model, image features are treated as the first ``word" in the caption, then passed to the LSTM cell. In the merge-concat model, image and caption features are processed separately, concatenated, then passed to a dense layer to form predictions.   The merge-add layer (not shown above) differs from the merge-concat model only in that image and caption features are \textit  {added} instead of \textit  {concatenated}.   Layer names above refer to \href  {https://keras.io/layers/about-keras-layers/}{\texttt  {keras} layer classes}.\relax }}{7}{figure.caption.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {V-E}Inference as Beam Search}{7}{subsection.5.5}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Inference: Greedy Selection\relax }}{8}{algorithm.1}}
\newlabel{greedy-sampling}{{1}{8}{Inference: Greedy Selection\relax }{algorithm.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Inference: Beam Search\relax }}{8}{algorithm.2}}
\newlabel{beam-search}{{2}{8}{Inference: Beam Search\relax }{algorithm.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}EXPERIMENTS}{8}{section.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {VI-A}Training details}{8}{subsection.6.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Top caption predictions using beam search and the inject model, for different values of $\alpha $. Small values of $\alpha $ lead to overly terse captions, while large values of $\alpha $ lead to wordy and repetitive captions.\relax }}{9}{figure.caption.8}}
\newlabel{fig:test1}{{8}{9}{Top caption predictions using beam search and the inject model, for different values of $\alpha $. Small values of $\alpha $ lead to overly terse captions, while large values of $\alpha $ lead to wordy and repetitive captions.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VII}Results}{9}{section.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {VII-A}Quantitative Analysis}{9}{subsection.7.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Training and validation losses for the inject, merge-concat, and merge-add models.\relax }}{9}{figure.caption.9}}
\newlabel{fig:test1}{{9}{9}{Training and validation losses for the inject, merge-concat, and merge-add models.\relax }{figure.caption.9}{}}
\newlabel{my-label}{{\caption@xref {my-label}{ on input line 773}}{10}{Quantitative Analysis}{table.caption.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Validation set prediction quality scores.\relax }}{10}{table.caption.10}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces \textbf  {Top-left:} BLEU-1 and ROUGE-L scores grouped by $\alpha $, to highlight differences between model architectures. \textbf  {Top-right:} Label frequency in validation set. \textbf  {Bottom:} BLEU-1 and ROUGE-L achieved by each model on validation examples grouped by label, split by value of $\alpha $.\relax }}{10}{figure.caption.12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {VII-B}Qualitative Analysis}{11}{subsection.7.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Words that lead training captions most frequenlty.\relax }}{11}{figure.caption.14}}
\newlabel{fig:test1}{{12}{11}{Words that lead training captions most frequenlty.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Sample images the final system predicts are ``chicken and waffles." The system predicts that around one in five images - often times or images that look nothing like the American dish. This behavior resembles a policy of \textit  {``when in doubt: chicken and waffles.''}\relax }}{12}{figure.caption.13}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Most common bigrams - given that the first word is \texttt  {"chicken"}.\relax }}{12}{figure.caption.15}}
\newlabel{fig:test1}{{13}{12}{Most common bigrams - given that the first word is \texttt {"chicken"}.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Most common trigrams - given that the first two words are \texttt  {"chicken and"}.\relax }}{12}{figure.caption.16}}
\newlabel{fig:test1}{{14}{12}{Most common trigrams - given that the first two words are \texttt {"chicken and"}.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VIII}CONCLUSION}{13}{section.8}}
\bibstyle{IEEEtran}
\bibdata{main}
\bibcite{vinyals}{1}
\bibcite{tanti1}{2}
\bibcite{tanti2}{3}
\bibcite{cho}{4}
\bibcite{kumar}{5}
\bibcite{xu}{6}
\bibcite{8k}{7}
\bibcite{30k}{8}
\bibcite{coco}{9}
\bibcite{wu}{10}
\bibcite{vgg}{11}
\bibcite{keras}{12}
\bibcite{imagenet}{13}
\bibcite{zipf}{14}
\bibcite{word2vec}{15}
\bibcite{bleu}{16}
\bibcite{rouge}{17}
\bibcite{cider}{18}
\@writefile{toc}{\contentsline {section}{References}{14}{section*.17}}
