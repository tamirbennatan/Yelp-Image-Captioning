\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{vinyals}
\citation{tanti1}
\citation{tanti2}
\citation{cho}
\citation{kumar}
\citation{xu}
\citation{8k}
\citation{30k}
\citation{coco}
\@writefile{toc}{\contentsline {section}{\numberline {I}INTRODUCTION}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}RELATED WORK}{1}{section.2}}
\newlabel{fig:test1}{{II}{2}{RELATED WORK}{section.2}{}}
\newlabel{fig:test2}{{II}{2}{RELATED WORK}{section.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The encoder-decoder framework in neural machine translation models (left) and image captioning models (right). In machine translation applications, an input sequence is encoded by an RNN to a vector represention, before decoded to an output sequence. Analogously, a neural image captioning system uses a CNN to encode an input image before generating an output sequence.}}{2}{figure.1}}
\@writefile{toc}{\contentsline {section}{\numberline {III}THEORETICAL BACKGROUND}{2}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {III-A}Training as Maximizing Caption Likelihood}{2}{subsection.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}METHODOLOGY}{2}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-A}Regularized Logistic Regression: Model Hyperparameters}{2}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-B}Feedforward Neural Network: Model Hyperparameters}{3}{subsection.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-C}Feedforward Neural Network: Learning Optimizations}{3}{subsection.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-D}Convolutional Neural Network: Hyperparameter Tuning}{3}{subsection.4.4}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Hyperparameter Ranges Explored for CNN models}}{3}{table.1}}
\newlabel{my-label}{{I}{3}{Hyperparameter Ranges Explored for CNN models}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Constrained Hyperparameter Ranges}}{3}{table.2}}
\newlabel{my-label}{{II}{3}{Constrained Hyperparameter Ranges}{table.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-E}CNN: Learning Optimization}{3}{subsection.4.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-F}CNN: Aggregated Predictions}{3}{subsection.4.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Learning curves for training periods with $\eta = .0001$ and $\eta = .00005$, with early stopping. Note: training accuracy is lower than validation accuracy. This is because we used data augmentation, and the new randomly rotated images made the training data "harder" to learn than the validation data.}}{4}{figure.2}}
\newlabel{figurelabel}{{2}{4}{Learning curves for training periods with $\eta = .0001$ and $\eta = .00005$, with early stopping. Note: training accuracy is lower than validation accuracy. This is because we used data augmentation, and the new randomly rotated images made the training data "harder" to learn than the validation data}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}RESULTS}{4}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {V-A}Regularized Logistic Regression}{4}{subsection.5.1}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces Best Logistic Regression Models, Trained with l1/l2 Loss}}{4}{table.3}}
\newlabel{my-}{{III}{4}{Best Logistic Regression Models, Trained with l1/l2 Loss}{table.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {V-B}Feedforward Neural Network}{4}{subsection.5.2}}
\@writefile{lot}{\contentsline {table}{\numberline {IV}{\ignorespaces FFNN Architectures}}{4}{table.4}}
\newlabel{my-label}{{IV}{4}{FFNN Architectures}{table.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {V}{\ignorespaces Validation Accuracies for FFNN Experiments}}{4}{table.5}}
\newlabel{my-label}{{V}{4}{Validation Accuracies for FFNN Experiments}{table.5}{}}
\bibstyle{IEEEtran}
\bibdata{main}
\bibcite{vinyals}{1}
\bibcite{tanti1}{2}
\@writefile{lot}{\contentsline {table}{\numberline {VI}{\ignorespaces Best CNN Models for Different Data Augmentation Schemes}}{5}{table.6}}
\newlabel{my-label}{{VI}{5}{Best CNN Models for Different Data Augmentation Schemes}{table.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {V-C}Convolutional Neural Network}{5}{subsection.5.3}}
\@writefile{lot}{\contentsline {table}{\numberline {VII}{\ignorespaces Highest performing CNN Models and Aggregations}}{5}{table.7}}
\newlabel{my-label}{{VII}{5}{Highest performing CNN Models and Aggregations}{table.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}DISCUSSION AND CONCLUSIONS}{5}{section.6}}
\@writefile{toc}{\contentsline {section}{References}{5}{section*.1}}
\bibcite{tanti2}{3}
\bibcite{cho}{4}
\bibcite{kumar}{5}
\bibcite{xu}{6}
\bibcite{8k}{7}
\bibcite{30k}{8}
\bibcite{coco}{9}
